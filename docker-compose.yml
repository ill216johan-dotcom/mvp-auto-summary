# MVP Auto-Summary Infrastructure
# Usage: docker compose up -d
# Docs: docs/SPECS.md

services:

  # ============================================================
  # n8n — workflow automation engine
  # UI: http://{VPS_IP}:5678
  # ============================================================
  n8n:
    image: n8nio/n8n:latest
    restart: unless-stopped
    ports:
      - "${N8N_PORT:-5678}:5678"
    volumes:
      - n8n_data:/home/node/.n8n
      - /mnt/recordings:/recordings:ro           # NFS mount (read-only)
      - ./scripts:/scripts:ro                     # Helper scripts
    environment:
      # --- Core ---
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
      - WEBHOOK_URL=${N8N_WEBHOOK_URL:-http://localhost:5678}
      - GENERIC_TIMEZONE=${TIMEZONE:-Europe/Moscow}
      - N8N_DEFAULT_LOCALE=ru
      - N8N_SECURE_COOKIE=false

      # --- Auth ---
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_USER:-admin}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD:?Set N8N_PASSWORD in .env}

      # --- Database (PostgreSQL) ---
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=${POSTGRES_DB:-n8n}
      - DB_POSTGRESDB_USER=${POSTGRES_USER:-n8n}
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD:?Set POSTGRES_PASSWORD in .env}

      # --- Security ---
      # Allow workflows to read env vars via $env (disabled by default in n8n)
      - N8N_BLOCK_ENV_ACCESS_IN_NODE=false

      # --- File access ---
      # Allow n8n to read files from /recordings (NFS mount)
      - N8N_ALLOWED_PATHS=/recordings,/home/node/.n8n-files

      # --- Nodes ---
      # Re-enable LocalFileTrigger (disabled by default in n8n 2.0)
      # We use Schedule+Scan instead, but keep it available as fallback
      - NODES_EXCLUDE=

      # --- Execution ---
      - EXECUTIONS_TIMEOUT=1800                  # 30 min in seconds
      - EXECUTIONS_TIMEOUT_MAX=3600              # 1 hour max in seconds

      # --- External API keys (accessible via $env in workflows) ---
      - WHISPER_URL=http://whisper:9000
      - GLM4_API_KEY=${GLM4_API_KEY}
      - GLM4_BASE_URL=${GLM4_BASE_URL:-https://api.z.ai/api/paas/v4}
      - GLM4_MODEL=${GLM4_MODEL:-glm-4.7-flashx}
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - TELEGRAM_CHAT_ID=${TELEGRAM_CHAT_ID}
      - OPEN_NOTEBOOK_URL=http://open-notebook:8888
      - OPEN_NOTEBOOK_TOKEN=${OPEN_NOTEBOOK_TOKEN:-password}
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - mvp-network

  # ============================================================
  # PostgreSQL — n8n metadata + processed files tracker
  # ============================================================
  postgres:
    image: postgres:16-alpine
    restart: unless-stopped
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql:ro
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-n8n}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:?Set POSTGRES_PASSWORD in .env}
      - POSTGRES_DB=${POSTGRES_DB:-n8n}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-n8n}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - mvp-network

  # ============================================================
  # Whisper — self-hosted Speech-to-Text (replaces Yandex SpeechKit)
  # Saves ~25,000 RUB/month, supports WebM natively
  # API: POST http://localhost:9000/asr (multipart file upload)
  # ============================================================
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest-cpu
    restart: unless-stopped
    ports:
      - "${WHISPER_PORT:-9000}:9000"
    environment:
      - ASR_MODEL=${WHISPER_MODEL:-medium}
      - ASR_ENGINE=faster_whisper
    volumes:
      - whisper_cache:/root/.cache/huggingface
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    networks:
      - mvp-network

  # ============================================================
  # SurrealDB — open-notebook database (graph + vector + fulltext)
  # ============================================================
  surrealdb:
    image: surrealdb/surrealdb:v2          # Pinned! latest может быть v3 (несовместим с open-notebook)
    restart: unless-stopped
    user: root
    command: start --user ${SURREAL_USER:-root} --pass ${SURREAL_PASSWORD:-changeme} surrealkv:/data/srdb.db
    volumes:
      - surrealdb_data:/data
    ports:
      - "${SURREAL_PORT:-8000}:8000"
    healthcheck:
      test: ["CMD-SHELL", "printf 'GET /health HTTP/1.0\\r\\n\\r\\n' | nc -w 2 localhost 8000 | grep -q '200' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 15s
    networks:
      - mvp-network

  # ============================================================
  # open-notebook — NotebookLM alternative with RAG
  # UI: http://{VPS_IP}:8888
  # API: http://{VPS_IP}:8888/api (see docs/API.md)
  # ============================================================
  open-notebook:
    image: lfnovo/open_notebook:v1-latest  # Pinned! v2+ может сломать API
    restart: unless-stopped
    ports:
      - "${NOTEBOOK_PORT:-8888}:8888"
    volumes:
      - notebook_data:/app/data
    environment:
      # --- Database ---
      # ВАЖНО: переменная называется SURREAL_URL (не SURREAL_ADDRESS!)
      # ВАЖНО: переменная называется SURREAL_PASSWORD (не SURREAL_PASS!)
      - SURREAL_URL=ws://surrealdb:8000/rpc
      - SURREAL_USER=${SURREAL_USER:-root}
      - SURREAL_PASSWORD=${SURREAL_PASSWORD:-changeme}
      - SURREAL_NAMESPACE=open_notebook
      - SURREAL_DATABASE=open_notebook

      # --- LLM Provider (GLM-4 via OpenRouter or Ollama) ---
      # Option A: ZhipuAI direct (OpenAI-compatible)
      - DEFAULT_MODEL=${NOTEBOOK_DEFAULT_MODEL:-openai/glm-4.7-flashx}
      - OPENAI_API_KEY=${GLM4_API_KEY:-not-needed}
      - OPENAI_BASE_URL=${GLM4_BASE_URL:-https://api.z.ai/api/paas/v4}

      # --- Embedding (for vector search / RAG) ---
      # Use a small embedding model; configure in UI Settings
      - DEFAULT_EMBEDDING_MODEL=${NOTEBOOK_EMBEDDING_MODEL:-}
    depends_on:
      surrealdb:
        condition: service_healthy    # Ждём пока SurrealDB готов принимать соединения
    networks:
      - mvp-network

volumes:
  n8n_data:
    driver: local
  postgres_data:
    driver: local
  surrealdb_data:
    driver: local
  notebook_data:
    driver: local
  whisper_cache:
    driver: local

networks:
  mvp-network:
    driver: bridge
